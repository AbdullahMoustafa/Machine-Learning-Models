{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the pandas library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading the data\n",
    "path = \"C:\\\\Users\\\\Abdullah\\\\Desktop\\\\bbc-fulltext-and-category\\\\bbc-text.csv\"\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'such', 'just', 'am', 'as', 'had', 'some', 'doing', 'be', 'an', 'shan', 'above', \"weren't\", 'ma', \"won't\", 'into', 'about', 'itself', 'a', 'ours', 'were', 'his', 'by', 'haven', 'other', 'does', \"isn't\", \"didn't\", \"mustn't\", \"shan't\", 'wouldn', 'until', \"wasn't\", 'in', 'we', 'he', 'it', 'weren', 'she', 'hasn', \"you've\", 'having', 've', 'couldn', 'i', \"doesn't\", 're', 'isn', 'to', 'each', 'they', 'where', \"hasn't\", 'didn', 'yourselves', \"don't\", 'against', 'below', 'few', 'here', 'ain', 'himself', 'or', 'at', 'these', 'mightn', 'o', 'themselves', \"needn't\", 'again', 'most', 'mustn', \"mightn't\", 'her', 'more', 'y', 'won', 'then', 'how', 'our', 'this', 'its', 'only', 'when', \"haven't\", 'm', 'll', 'wasn', 'of', 'have', 'now', 'any', 'did', \"you'd\", 'ourselves', 'than', 'hadn', 'them', 'can', 'same', 'their', 'with', 'through', 'no', 't', \"hadn't\", \"she's\", 'those', 'not', 'if', 'during', 'are', 'theirs', 'that', \"that'll\", 'but', 'under', 'between', 'needn', \"wouldn't\", 'before', 'on', 'and', 'herself', 'hers', 'so', \"aren't\", 'is', 'nor', 'yourself', 'him', 'do', 'for', \"you'll\", 'was', 'you', 'shouldn', 'been', 'why', 'up', 'aren', 'own', \"it's\", 'has', 'yours', 'will', 'the', \"should've\", 'your', 'whom', 'what', 'don', 'there', 'once', 'who', 'all', 'further', 'myself', \"shouldn't\", 'doesn', 'which', 'my', 'down', 'me', 'being', 'while', 'too', 'because', 'd', 'off', 's', 'after', 'both', 'very', 'over', 'out', 'from', \"you're\", \"couldn't\", 'should'}\n"
     ]
    }
   ],
   "source": [
    "# loading the stop words list, stemmer and lemmatizer\n",
    "stop_words =  {'which', 'and', \"that'll\", 'have', 'now', 'our', 'herself', \"don't\", \"mustn't\", 'too', 'or', 'to', 'yours', 'what', \"you'd\", 'these', 'theirs', 'myself', \"you'll\", 'through', 'hasn', \"you're\", 't', 'where', 'mustn', \"shouldn't\", 'into', 'during', 'itself', 'can', 'y', 'his', 'after', 'any', 'be', 'she', 'was', 'ain', 'above', 'does', 'doing', 'between', 'some', \"hasn't\", \"shan't\", 'under', 'nor', 'her', \"won't\", 'further', 'has', \"didn't\", 'needn', \"wouldn't\", 'it', 'having', 'with', 'few', 'at', 'out', 'on', 'because', 'm', 'more', 'there', 'themselves', 'that', 'don', 'did', 'an', 'off', 'you', 'up', 's', 'haven', 'down', 're', 'a', 'didn', 'their', 'them', 'no', 'other', 'him', 'shan', 'until', 'very', \"hadn't\", 'me', 'how', 'such', 'ma', 'in', \"weren't\", 'my', 'just', 'against', 'this', 'own', 'should', 'if', 'yourselves', 'the', 'aren', 'd', 'here', 'before', 'ourselves', 'were', 'while', 'doesn', 'yourself', 'below', 'of', 'is', 'so', \"couldn't\", 'each', \"needn't\", 've', 'when', 'both', 'couldn', 'most', 'wouldn', 'by', 'll', \"she's\", 'as', 'we', 'won', 'isn', 'once', 'wasn', 'again', 'will', 'ours', 'o', \"aren't\", 'its', 'who', 'than', 'i', 'whom', \"you've\", 'hers', \"it's\", 'only', 'hadn', 'are', 'they', 'shouldn', 'for', \"should've\", 'those', 'himself', \"doesn't\", \"isn't\", 'mightn', \"haven't\", 'weren', 'all', 'am', 'but', 'same', 'then', 'from', 'why', 'do', \"wasn't\", 'he', 'about', 'not', \"mightn't\", 'your', 'had', 'over', 'been', 'being'}\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(text):\n",
    "    # tokenize the text\n",
    "    words = text.split()\n",
    "    new_words_list = []\n",
    "    \n",
    "    for word in words:\n",
    "        # only add words which are not stop words\n",
    "        if word not in stop_words:\n",
    "            new_words_list.append(word)\n",
    "    \n",
    "    # concatenate the string\n",
    "    return \" \".join(new_words_list)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # replace non-alphabets with null\n",
    "    text = re.sub('[^a-zA-Z ]','',text)\n",
    "    \n",
    "    #remove stop words\n",
    "    text = process_words(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text before Pre-Processing:\n",
      " yeading face newcastle in fa cup premiership side newcastle united face a trip to ryman premier league leaders yeading in the fa cup third round.  the game - arguably the highlight of the draw - is a potential money-spinner for non-league yeading  who beat slough in the second round. conference side exeter city  who knocked out doncaster on saturday  will travel to old trafford to meet holders manchester united in january. arsenal were drawn at home to stoke and chelsea will play host to scunthorpe. the only other non-league side in the draw are hinckley united  who held brentford to a goalless draw on sunday. they will meet league one leaders luton if they win their replay against martin allen s team at griffin park.  a number of premiership teams face difficult away games against championship sides on the weekend of 8/9 january. third-placed everton visit plymouth  liverpool travel to burnley  crystal palace go to sunderland  fulham face carling cup semi-finalists watford  bolton meet ipswich  while aston villa were drawn against sheffield united. premiership strugglers norwich  blackburn  west brom are away at west ham  cardiff and preston north end respectively. southampton visit northampton  having already beaten the league two side in the carling cup earlier this season. middlesbrough were drawn away against either swindon or notts county  while spurs entertain brighton at white hart lane.  arsenal v stoke  swindon/notts co v middlesbrough  man utd v exeter  plymouth v everton  leicester v blackpool  derby v wigan  sunderland v crystal palace  wolves v millwall  yeading v newcastle  hull v colchester  tottenham v brighton  reading v stockport/swansea  birmingham v leeds  hartlepool v boston  milton keynes dons v peterborough  oldham v man city  chelsea v scunthorpe  cardiff v blackburn  charlton v rochdale  west ham v norwich  sheff utd v aston villa  preston v west brom  rotherham v yeovil  burnley v liverpool  bournemouth v chester  coventry v crewe  watford v fulham  ipswich v bolton  portsmouth v gillingham  northampton v southampton  qpr v nottm forest  luton v hinckley/brentford  matches to be played on weekend of 8/9 january.\n"
     ]
    }
   ],
   "source": [
    "sample = data['text'][3]\n",
    "\n",
    "print(\"Sample Text before Pre-Processing:\\n\",sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text after Pre-Processing:\n",
      " yeading face newcastle fa cup premiership side newcastle united face trip ryman premier league leaders yeading fa cup third round game arguably highlight draw potential moneyspinner nonleague yeading beat slough second round conference side exeter city knocked doncaster saturday travel old trafford meet holders manchester united january arsenal drawn home stoke chelsea play host scunthorpe nonleague side draw hinckley united held brentford goalless draw sunday meet league one leaders luton win replay martin allen team griffin park number premiership teams face difficult away games championship sides weekend january thirdplaced everton visit plymouth liverpool travel burnley crystal palace go sunderland fulham face carling cup semifinalists watford bolton meet ipswich aston villa drawn sheffield united premiership strugglers norwich blackburn west brom away west ham cardiff preston north end respectively southampton visit northampton already beaten league two side carling cup earlier season middlesbrough drawn away either swindon notts county spurs entertain brighton white hart lane arsenal v stoke swindonnotts co v middlesbrough man utd v exeter plymouth v everton leicester v blackpool derby v wigan sunderland v crystal palace wolves v millwall yeading v newcastle hull v colchester tottenham v brighton reading v stockportswansea birmingham v leeds hartlepool v boston milton keynes dons v peterborough oldham v man city chelsea v scunthorpe cardiff v blackburn charlton v rochdale west ham v norwich sheff utd v aston villa preston v west brom rotherham v yeovil burnley v liverpool bournemouth v chester coventry v crewe watford v fulham ipswich v bolton portsmouth v gillingham northampton v southampton qpr v nottm forest luton v hinckleybrentford matches played weekend january\n"
     ]
    }
   ],
   "source": [
    "pre_sample = preprocess(sample)\n",
    "print(\"Sample Text after Pre-Processing:\\n\",pre_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    tv future hands viewers home theatre systems p...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# apply preprocessing on entire dataset\n",
    "x = data['text'].apply(lambda x:preprocess(x))\n",
    "print(x[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the input text to suitable features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "x_counts = count_vectorizer.fit_transform(x)\n",
    "x_tfidf = tfidf_vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = x_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encodings: [4 0 3 ... 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# create labels for target\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder \n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform( data['category'] )\n",
    "print(\"Label Encodings:\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training data: 1891\n",
      "Size of Testing data: 334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_x,y,test_size=0.15)\n",
    "print(\"Size of Training data:\",x_train.shape[0])\n",
    "print(\"Size of Testing data:\",x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB()\n",
    "logistic_model = LogisticRegression()\n",
    "dec_tree_model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "nb_model.fit(x_train,y_train)\n",
    "logistic_model.fit(x_train,y_train)\n",
    "dec_tree_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9700598802395209\n",
      "Logistic Regression Accuracy: 0.9790419161676647\n",
      "Desicion Tree Accuracy: 0.8622754491017964\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(model,x_test,y_test):\n",
    "    total = x_test.shape[0]\n",
    "    count =  0\n",
    "    res = model.predict(x_test)\n",
    "    for i in range(total):\n",
    "        y_true = y_test[i]\n",
    "        if y_true == res[i]:\n",
    "            count+=1\n",
    "    return count/total\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\",check_accuracy(nb_model,x_test,y_test))\n",
    "print(\"Logistic Regression Accuracy:\",check_accuracy(logistic_model,x_test,y_test))\n",
    "print(\"Desicion Tree Accuracy:\",check_accuracy(dec_tree_model,x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Result: ['entertainment']\n",
      "Logistic Regession Result: ['sport']\n",
      "Decision Tree Result: ['sport']\n"
     ]
    }
   ],
   "source": [
    "sample_data = \"actor injured while shooting and the movie got cancelled\"\n",
    "processed_data = preprocess(sample_data)\n",
    "x_sample = count_vectorizer.transform([processed_data])\n",
    "\n",
    "# get results\n",
    "nb_result =  nb_model.predict(x_sample)\n",
    "log_result = logistic_model.predict(x_sample)\n",
    "tree_result = dec_tree_model.predict(x_sample)\n",
    "\n",
    "# get labels\n",
    "nb_result = label_encoder.inverse_transform(nb_result)\n",
    "log_result = label_encoder.inverse_transform(log_result)\n",
    "tree_result = label_encoder.inverse_transform(tree_result)\n",
    "\n",
    "print(\"Naive Bayes Result:\",nb_result)\n",
    "print(\"Logistic Regession Result:\",log_result)\n",
    "print(\"Decision Tree Result:\",tree_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: 30174\n",
      "Output Shape: 5\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train[0].shape[1]\n",
    "output_shape = 5\n",
    "print(\"Input Shape:\",input_shape)\n",
    "print(\"Output Shape:\",output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot Vector:\n",
      " [[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#create one hot encoding's for the labels\n",
    "onehotencoder = OneHotEncoder() \n",
    "onehotencoder.fit(y.reshape(-1, 1))\n",
    "labels = onehotencoder.transform(y.reshape(-1, 1)).toarray()\n",
    "print(\"One hot Vector:\\n\",labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(new_x,labels,test_size=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense( 1024 , activation='sigmoid',  input_dim = input_shape  ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense( 512  , activation='sigmoid' ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense( 256   , activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128   , activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense( 64   , activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense( 5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 1024)              30899200  \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 31,596,805\n",
      "Trainable params: 31,596,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1891 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      "1891/1891 [==============================] - 82s 43ms/step - loss: 1.5824 - accuracy: 0.2734 - val_loss: 1.1938 - val_accuracy: 0.6976\n",
      "Epoch 2/10\n",
      "1824/1891 [===========================>..] - ETA: 1s - loss: 0.7272 - accuracy: 0.8218"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train, epochs = 10, batch_size=32, validation_data = (x_test,y_test)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "h = history\n",
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = \"actor injured while shooting and the movie got cancelled\"\n",
    "processed_data = preprocess(sample_data)\n",
    "x_sample = count_vectorizer.transform([processed_data])\n",
    "\n",
    "# get results\n",
    "model_result = list(model.predict(x_sample)[0])\n",
    "\n",
    "print(model_result)\n",
    "\n",
    "# get labels\n",
    "max_ = max(model_result)\n",
    "index = model_result.index(max_)\n",
    "result = label_encoder.inverse_transform([index])\n",
    "\n",
    "print(\"Model Result:\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
